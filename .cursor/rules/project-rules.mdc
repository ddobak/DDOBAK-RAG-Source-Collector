---
description: 
globs: 
alwaysApply: true
---
# DDOBAK RAG Source Collector - 프로젝트 규칙

## 프로젝트 개요
Poetry 기반 다중 사이트 웹 크롤링 프로젝트입니다. 클린 코드와 클린 아키텍처 원칙을 따라 각 사이트별로 모듈화된 크롤러를 구현합니다.

## 프로젝트 구조

```
DDOBAK-RAG-Source-Collector/
├── main.py                     # 애플리케이션 진입점 (핵심 흐름만)
├── pyproject.toml             # Poetry 설정 및 크롤러 entry points 정의  
├── config.py                  # 설정 관리 (.env 포함)
├── .env.example               # 환경변수 예시 파일
├── .env                       # 실제 환경변수 (gitignore)
├── README.md                  # 프로젝트 메인 문서
├── docs/                      # 프로젝트 문서화
│   ├── architecture.md        # 아키텍처 설명
│   ├── data-structure.md      # 데이터 구조 설명
│   ├── setup.md              # 설정 및 설치 가이드
│   └── crawler-guide.md      # 크롤러 개발 가이드
├── data/                      # 크롤링된 데이터 저장
│   └── lawtalk/              # 로톡 데이터
│       ├── consultation_case/ # 상담 사례
│       ├── solved_cases/     # 해결된 사례
│       └── guide_posts/      # 가이드 포스트
├── logs/                      # 로그 파일
├── lawtalk/                   # 로톡 크롤러 모듈
│   ├── __init__.py
│   ├── lawtalk_crawler.py    # 메인 크롤러
│   ├── lawtalk_config.py     # 로톡 설정
│   └── cases/                # 사례별 크롤링 모듈
│       ├── get_qna_data.py   # 상담 사례 크롤링
│       ├── get_solved_data.py # 해결된 사례 크롤링
│       └── get_guide_data.py # 가이드 포스트 크롤링
├── law_open_api/              # 법무부 오픈API 크롤러 모듈
│   ├── __init__.py
│   └── api_crawler.py
├── easylaw/                   # 이지로 크롤러 모듈
│   ├── __init__.py
│   └── crawler.py
├── common/                    # 공통 유틸리티
│   ├── __init__.py
│   ├── base_crawler.py        # 크롤러 베이스 클래스
│   └── crawler_registry.py    # Poetry entry points 기반 크롤러 관리
├── utils/                     # 유틸리티 함수들
│   ├── __init__.py
│   ├── utils.py              # 로컬 파일 처리 유틸리티
│   ├── s3.py                 # S3 관련 유틸리티
│   └── validate_args.py      # CLI 인자 검증
└── tests/                     # 테스트 코드
    ├── __init__.py
    └── test_lawtalk.py
```

## 핵심 파일 설명

### 진입점과 설정
- **main.py**: CLI 인자를 받아 크롤러를 실행하는 메인 진입점 (핵심 흐름만 포함)
- **config.py**: 환경변수와 설정값들을 관리하는 중앙 설정 파일
- **pyproject.toml**: Poetry 의존성 관리 및 크롤러 entry points 정의

### 크롤러 모듈
- **lawtalk/**: 로톡 크롤러들을 정리한 모듈
- **law_open_api/**: 법무부 오픈API 크롤러 모듈  
- **easylaw/**: 이지로 크롤러 모듈

### 공통 모듈
- **common/base_crawler.py**: 모든 크롤러가 상속받는 베이스 클래스
- **common/crawler_registry.py**: Poetry entry points 기반 크롤러 관리
- **utils/**: 프로젝트 전반에서 사용되는 유틸리티 함수들

## 아키텍처 특징

### 1. Poetry Entry Points 기반 플러그인 시스템
- 크롤러 등록은 `pyproject.toml`의 `[tool.poetry.plugins."ddobak.crawlers"]`에서 관리
- 새로운 크롤러 추가시 `pyproject.toml`만 수정하면 자동 인식
- 동적 로딩으로 확장성과 유지보수성 확보

### 2. 관심사 분리
- **main.py**: 핵심 진행 흐름만 (간결한 CLI 인터페이스)
- **crawler_registry.py**: 크롤러 관리 로직 분리
- **config.py**: 순수 설정값만 관리
- **utils/**: 기능별로 분리된 유틸리티 모듈들

### 3. 상속 기반 구조
- 모든 크롤러는 `BaseCrawler`를 상속
- 공통 기능 (로깅, 출력 디렉토리 설정) 자동 제공
- `get_site_name()`, `crawl()` 메서드만 구현하면 됨

## 개발 규칙

### 1. 언어 규칙
- **모든 설명과 문서는 한국어로 작성**
- **코드 주석은 간결한 한국어로 작성**
- **로깅 메시지는 영어로 작성**

### 2. 코드 품질 규칙
- **클린 코드와 클린 아키텍처 원칙 준수**
- **목적, 책임, 기능별로 모듈화**
- **높은 가독성과 재사용성 유지**
- **90% 이상 확신이 있을 때만 코드 수정**
- **git diff를 최소화하는 변경**

### 3. Poetry 사용 규칙
- **Python 3.11 사용**
- **모든 실행은 `poetry run` 기반**
- **테스트 스크립트: `poetry run test-{site_name}`**
- **실행 스크립트: `poetry run start {site_name}`**
- **최신 호환 패키지 버전 사용**

### 4. 아키텍처 규칙
- **각 사이트별 독립적인 크롤러 모듈**
- **모든 크롤러는 BaseCrawler 상속**
- **공통 기능은 common/ 모듈에 구현**
- **Poetry entry points로 크롤러 등록**
- **requests 또는 selenium 유연하게 선택**
- **환경변수를 통한 민감 정보 관리**

### 5. 패키지 관리
- **beautifulsoup4**: HTML 파싱용 (최신 버전)
- **requests**: HTTP 클라이언트용 (최신 버전)  
- **selenium**: 동적 페이지 크롤링용 (필요시)
- **python-dotenv**: 환경변수 관리용
- **click**: CLI 인터페이스용
- **pytest**: 테스트 프레임워크

### 6. 실행 패턴
```bash
# 특정 사이트 크롤링 실행
poetry run start lawtalk
poetry run start law_open_api  
poetry run start easylaw

# 테스트 실행
poetry run test-lawtalk
poetry run test-law_open_api
poetry run test-easylaw
```

### 7. 크롤러 개발 규칙
```python
# 새 크롤러 구현 예시
from common.base_crawler import BaseCrawler

class NewSiteCrawler(BaseCrawler):
    def get_site_name(self) -> str:
        return "new_site"
    
    def crawl(self) -> None:
        # 크롤링 로직 구현
        pass
```

### 8. 파일 네이밍 규칙
- **크롤러 파일**: `{기능}_crawler.py`
- **테스트 파일**: `test_{모듈명}.py`
- **스네이크 케이스 사용**

### 9. Import 규칙
```python
# 표준 라이브러리
import os
import sys

# 서드파티 라이브러리  
import requests
from bs4 import BeautifulSoup

# 로컬 모듈
from common.base_crawler import BaseCrawler
from config import config
```

### 10. 에러 처리
- **명확한 예외 처리와 로깅**
- **영어로 된 로그 메시지**
- **사용자 친화적인 한국어 에러 메시지**

### 11. 테스트 규칙
- **각 크롤러별 단위 테스트 작성**
- **pytest 사용**
- **실제 네트워크 호출 활용해 확실한 검증**

## 새 크롤러 추가 방법

1. **크롤러 클래스 생성**: `BaseCrawler`를 상속받아 구현
2. **pyproject.toml 등록**: `[tool.poetry.plugins."ddobak.crawlers"]`에 entry point 추가
3. **테스트 작성**: `tests/` 디렉토리에 테스트 파일 생성
4. **설정 추가**: 필요시 `config.py`에 사이트별 설정 추가
