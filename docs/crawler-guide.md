# í¬ë¡¤ëŸ¬ ê°œë°œ ê°€ì´ë“œ

DDOBAK RAG Source Collectorì—ì„œ ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ë¥¼ ê°œë°œí•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ìƒì„¸í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ—ï¸ í¬ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜ ê°œìš”

### BaseCrawler ì¶”ìƒ í´ë˜ìŠ¤

ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” `BaseCrawler`ë¥¼ ìƒì†ë°›ì•„ êµ¬í˜„í•©ë‹ˆë‹¤:

```python
from abc import ABC, abstractmethod
from common.base_crawler import BaseCrawler

class YourCrawler(BaseCrawler):
    @abstractmethod
    def get_site_name(self) -> str:
        """ì‚¬ì´íŠ¸ ì´ë¦„ ë°˜í™˜"""
        pass
    
    @abstractmethod  
    def crawl(self) -> None:
        """í¬ë¡¤ë§ ë¡œì§ êµ¬í˜„"""
        pass
```

### ê³µí†µ ê¸°ëŠ¥

BaseCrawlerì—ì„œ ì œê³µí•˜ëŠ” ê³µí†µ ê¸°ëŠ¥ë“¤:

- **ë¡œê¹…**: ìë™ ì„¤ì •ëœ ë¡œê±° (`self.logger`)
- **ì¶œë ¥ ë””ë ‰í„°ë¦¬**: ìë™ ìƒì„±ëœ ì¶œë ¥ ê²½ë¡œ (`self.output_dir`)
- **í¬ë¡¤ë§ ì˜µì…˜**: ì„¤ì • ì˜µì…˜ ê´€ë¦¬ (`self.crawl_options`)
- **ì‹¤í–‰ íë¦„**: í‘œì¤€í™”ëœ ì‹¤í–‰ íë¦„ (`run()` ë©”ì„œë“œ)

## ğŸ”§ ìƒˆ í¬ë¡¤ëŸ¬ ê°œë°œ ë‹¨ê³„

### 1. í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •

```bash
# ìƒˆ ì‚¬ì´íŠ¸ ëª¨ë“ˆ ìƒì„±
mkdir new_site
cd new_site

# í•„ìˆ˜ íŒŒì¼ ìƒì„±
touch __init__.py
touch crawler.py
touch config.py
```

### 2. í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ êµ¬í˜„

#### ê¸°ë³¸ êµ¬ì¡°

```python
# new_site/crawler.py
from typing import Dict, Any, Optional
import requests
from bs4 import BeautifulSoup

from common.base_crawler import BaseCrawler
from utils.utils import save_json_data_to_local
from .config import NewSiteConfig


class NewSiteCrawler(BaseCrawler):
    """ìƒˆ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, crawl_options: Optional[Dict[str, Any]] = None):
        super().__init__(crawl_options)
        self.config = NewSiteConfig()
        self.session = requests.Session()
        
    def get_site_name(self) -> str:
        """ì‚¬ì´íŠ¸ ì´ë¦„ ë°˜í™˜"""
        return "new_site"
    
    def crawl(self) -> None:
        """í¬ë¡¤ë§ ë©”ì¸ ë¡œì§"""
        self.logger.info("Starting new site crawling")
        
        try:
            # 1. ë¡œê·¸ì¸ (í•„ìš”í•œ ê²½ìš°)
            self._login()
            
            # 2. ë°ì´í„° ìˆ˜ì§‘
            data = self._fetch_data()
            
            # 3. ë°ì´í„° ì €ì¥
            self._save_data(data)
            
            self.logger.info("Crawling completed successfully")
            
        except Exception as e:
            self.logger.error(f"Crawling failed: {e}")
            raise
        finally:
            # 4. ì •ë¦¬ ì‘ì—…
            self._cleanup()
    
    def _login(self) -> None:
        """ë¡œê·¸ì¸ ì²˜ë¦¬"""
        # ë¡œê·¸ì¸ ë¡œì§ êµ¬í˜„
        pass
    
    def _fetch_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘"""
        # ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ êµ¬í˜„
        return {}
    
    def _save_data(self, data: Dict[str, Any]) -> None:
        """ë°ì´í„° ì €ì¥"""
        # ì €ì¥ ë¡œì§ êµ¬í˜„
        pass
    
    def _cleanup(self) -> None:
        """ì •ë¦¬ ì‘ì—…"""
        # ì„¸ì…˜ ì •ë¦¬ ë“±
        if hasattr(self, 'session'):
            self.session.close()
```

### 3. ì„¤ì • í´ë˜ìŠ¤ êµ¬í˜„

```python
# new_site/config.py
import os
from typing import Dict


class NewSiteConfig:
    """ìƒˆ ì‚¬ì´íŠ¸ ì„¤ì •"""
    
    # ê¸°ë³¸ URL
    BASE_URL = "https://example.com"
    API_URL = f"{BASE_URL}/api"
    
    # ì¸ì¦ ì •ë³´ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    USERNAME = os.getenv("NEW_SITE_USERNAME")
    PASSWORD = os.getenv("NEW_SITE_PASSWORD")
    
    # í¬ë¡¤ë§ ì„¤ì •
    REQUEST_INTERVAL = 1  # ìš”ì²­ ê°„ê²© (ì´ˆ)
    MAX_RETRIES = 3       # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    TIMEOUT = 30          # ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    # ë°ì´í„° ì„¤ì •
    DEFAULT_LIMIT = 10    # ê¸°ë³¸ ìˆ˜ì§‘ ê°œìˆ˜
    MAX_LIMIT = 100       # ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
    
    # í—¤ë” ì„¤ì •
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
    }
    
    @classmethod
    def validate_config(cls) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        if not cls.USERNAME or not cls.PASSWORD:
            return False
        return True
```

### 4. Poetry Entry Point ë“±ë¡

```toml
# pyproject.tomlì— ì¶”ê°€
[tool.poetry.plugins."ddobak.crawlers"]
new_site = "new_site.crawler:NewSiteCrawler"
```

### 5. í…ŒìŠ¤íŠ¸ ì‘ì„±

```python
# tests/test_new_site.py
import pytest
from unittest.mock import Mock, patch

from new_site.crawler import NewSiteCrawler
from new_site.config import NewSiteConfig


class TestNewSiteCrawler:
    """ìƒˆ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.crawler = NewSiteCrawler()
    
    def test_get_site_name(self):
        """ì‚¬ì´íŠ¸ ì´ë¦„ í…ŒìŠ¤íŠ¸"""
        assert self.crawler.get_site_name() == "new_site"
    
    def test_config_validation(self):
        """ì„¤ì • ìœ íš¨ì„± í…ŒìŠ¤íŠ¸"""
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • í›„ í…ŒìŠ¤íŠ¸
        with patch.dict('os.environ', {
            'NEW_SITE_USERNAME': 'test_user',
            'NEW_SITE_PASSWORD': 'test_pass'
        }):
            config = NewSiteConfig()
            assert config.validate_config() is True
    
    @patch('requests.Session.get')
    def test_fetch_data(self, mock_get):
        """ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        # Mock ì‘ë‹µ ì„¤ì •
        mock_response = Mock()
        mock_response.json.return_value = {"test": "data"}
        mock_response.status_code = 200
        mock_get.return_value = mock_response
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        data = self.crawler._fetch_data()
        
        # ê²€ì¦
        assert "test" in data
        mock_get.assert_called_once()
    
    def test_crawl_integration(self):
        """í†µí•© í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
        # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ í…ŒìŠ¤íŠ¸
        # ì£¼ì˜: ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ Mock ì‚¬ìš©
        pass
```

## ğŸ“š êµ¬í˜„ ì˜ˆì‹œ

### 1. ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬

```python
class SimpleWebCrawler(BaseCrawler):
    """ê°„ë‹¨í•œ ì›¹í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def get_site_name(self) -> str:
        return "simple_web"
    
    def crawl(self) -> None:
        url = "https://example.com/data"
        
        # HTTP ìš”ì²­
        response = requests.get(url)
        response.raise_for_status()
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë°ì´í„° ì¶”ì¶œ
        items = []
        for element in soup.find_all('div', class_='data-item'):
            item = {
                'title': element.find('h2').text.strip(),
                'content': element.find('p').text.strip(),
                'url': element.find('a')['href']
            }
            items.append(item)
        
        # ë°ì´í„° ì €ì¥
        data = {
            'success': True,
            'total_fetched': len(items),
            'data': {'items': items}
        }
        
        file_path = f"{self.get_site_name()}/data.json"
        save_json_data_to_local(data, self.output_dir, file_path)
```

### 2. API ê¸°ë°˜ í¬ë¡¤ëŸ¬

```python
class APICrawler(BaseCrawler):
    """API ê¸°ë°˜ í¬ë¡¤ëŸ¬"""
    
    def get_site_name(self) -> str:
        return "api_site"
    
    def crawl(self) -> None:
        # API ì¸ì¦
        auth_token = self._authenticate()
        
        # í—¤ë” ì„¤ì •
        headers = {
            "Authorization": f"Bearer {auth_token}",
            "Content-Type": "application/json"
        }
        
        # í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ë°ì´í„° ìˆ˜ì§‘
        all_data = []
        page = 1
        
        while True:
            # API ìš”ì²­
            response = requests.get(
                f"https://api.example.com/data?page={page}",
                headers=headers
            )
            
            if response.status_code != 200:
                break
                
            data = response.json()
            items = data.get('items', [])
            
            if not items:
                break
                
            all_data.extend(items)
            page += 1
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(self.config.REQUEST_INTERVAL)
        
        # ë°ì´í„° ì €ì¥
        result = {
            'success': True,
            'total_fetched': len(all_data),
            'data': {'items': all_data}
        }
        
        file_path = f"{self.get_site_name()}/api_data.json"
        save_json_data_to_local(result, self.output_dir, file_path)
    
    def _authenticate(self) -> str:
        """API ì¸ì¦"""
        auth_data = {
            'username': self.config.USERNAME,
            'password': self.config.PASSWORD
        }
        
        response = requests.post(
            "https://api.example.com/auth",
            json=auth_data
        )
        
        response.raise_for_status()
        return response.json()['access_token']
```

### 3. Selenium ê¸°ë°˜ í¬ë¡¤ëŸ¬

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class SeleniumCrawler(BaseCrawler):
    """Selenium ê¸°ë°˜ ë™ì  í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, crawl_options=None):
        super().__init__(crawl_options)
        self.driver = None
    
    def get_site_name(self) -> str:
        return "dynamic_site"
    
    def crawl(self) -> None:
        try:
            # ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”
            self._setup_driver()
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get("https://dynamic-example.com")
            
            # ë¡œê·¸ì¸ (í•„ìš”í•œ ê²½ìš°)
            self._login()
            
            # ë™ì  ì½˜í…ì¸  ëŒ€ê¸°
            wait = WebDriverWait(self.driver, 10)
            
            # ë°ì´í„° ìˆ˜ì§‘
            items = []
            elements = wait.until(
                EC.presence_of_all_elements_located(
                    (By.CLASS_NAME, "data-item")
                )
            )
            
            for element in elements:
                item = {
                    'title': element.find_element(By.TAG_NAME, 'h2').text,
                    'content': element.find_element(By.TAG_NAME, 'p').text
                }
                items.append(item)
            
            # ë°ì´í„° ì €ì¥
            data = {
                'success': True,
                'total_fetched': len(items),
                'data': {'items': items}
            }
            
            file_path = f"{self.get_site_name()}/dynamic_data.json"
            save_json_data_to_local(data, self.output_dir, file_path)
            
        finally:
            self._cleanup()
    
    def _setup_driver(self) -> None:
        """ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        self.driver = webdriver.Chrome(options=options)
    
    def _login(self) -> None:
        """ë¡œê·¸ì¸ ì²˜ë¦¬"""
        username_field = self.driver.find_element(By.NAME, "username")
        password_field = self.driver.find_element(By.NAME, "password")
        
        username_field.send_keys(self.config.USERNAME)
        password_field.send_keys(self.config.PASSWORD)
        
        login_button = self.driver.find_element(By.XPATH, "//button[@type='submit']")
        login_button.click()
        
        # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "dashboard"))
        )
    
    def _cleanup(self) -> None:
        """ì •ë¦¬ ì‘ì—…"""
        if self.driver:
            self.driver.quit()
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### 1. ì¦ë¶„ í¬ë¡¤ë§ êµ¬í˜„

```python
class IncrementalCrawler(BaseCrawler):
    """ì¦ë¶„ í¬ë¡¤ë§ì„ ì§€ì›í•˜ëŠ” í¬ë¡¤ëŸ¬"""
    
    def crawl(self) -> None:
        # ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ í™•ì¸
        last_crawl_time = self._get_last_crawl_time()
        only_new = self.crawl_options.get('only_new', False)
        
        if only_new and last_crawl_time:
            self.logger.info(f"Incremental crawling since {last_crawl_time}")
            data = self._fetch_new_data(last_crawl_time)
        else:
            self.logger.info("Full crawling")
            data = self._fetch_all_data()
        
        # í˜„ì¬ ì‹œê°„ì„ ì €ì¥
        self._save_crawl_time()
        
        # ë°ì´í„° ì €ì¥
        self._save_data(data)
    
    def _get_last_crawl_time(self) -> Optional[datetime]:
        """ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ ì¡°íšŒ"""
        try:
            with open(f"{self.output_dir}/.last_crawl", 'r') as f:
                timestamp = f.read().strip()
                return datetime.fromisoformat(timestamp)
        except FileNotFoundError:
            return None
    
    def _save_crawl_time(self) -> None:
        """í¬ë¡¤ë§ ì‹œê°„ ì €ì¥"""
        os.makedirs(self.output_dir, exist_ok=True)
        with open(f"{self.output_dir}/.last_crawl", 'w') as f:
            f.write(datetime.now().isoformat())
```

### 2. ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„

```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor


class ParallelCrawler(BaseCrawler):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” í¬ë¡¤ëŸ¬"""
    
    async def crawl_async(self) -> None:
        """ë¹„ë™ê¸° í¬ë¡¤ë§"""
        urls = self._get_urls_to_crawl()
        
        async with aiohttp.ClientSession() as session:
            tasks = [self._fetch_url(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        successful_results = [r for r in results if not isinstance(r, Exception)]
        self._save_data(successful_results)
    
    async def _fetch_url(self, session: aiohttp.ClientSession, url: str) -> Dict:
        """ê°œë³„ URL í¬ë¡¤ë§"""
        try:
            async with session.get(url) as response:
                data = await response.json()
                return {'url': url, 'data': data, 'success': True}
        except Exception as e:
            self.logger.error(f"Failed to fetch {url}: {e}")
            return {'url': url, 'error': str(e), 'success': False}
    
    def crawl(self) -> None:
        """ë™ê¸° í¬ë¡¤ë§ (ë¹„ë™ê¸° í˜¸ì¶œ)"""
        asyncio.run(self.crawl_async())
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë²” ì‚¬ë¡€

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
class TestCrawlerMethods:
    """í¬ë¡¤ëŸ¬ ë©”ì„œë“œë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
    
    @patch('requests.get')
    def test_fetch_data_success(self, mock_get):
        """ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Given
        mock_response = Mock()
        mock_response.json.return_value = {'data': 'test'}
        mock_response.status_code = 200
        mock_get.return_value = mock_response
        
        crawler = YourCrawler()
        
        # When
        result = crawler._fetch_data()
        
        # Then
        assert result['data'] == 'test'
        mock_get.assert_called_once()
    
    @patch('requests.get')
    def test_fetch_data_failure(self, mock_get):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸"""
        # Given
        mock_get.side_effect = requests.RequestException("Network error")
        crawler = YourCrawler()
        
        # When & Then
        with pytest.raises(requests.RequestException):
            crawler._fetch_data()
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸

```python
class TestCrawlerIntegration:
    """í¬ë¡¤ëŸ¬ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def test_full_crawl_process(self):
        """ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
        crawler = YourCrawler({
            'simple_result': True,
            'storage_type': 'local',
            'only_new': False
        })
        
        # ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ í™˜ê²½)
        crawler.crawl()
        
        # ê²°ê³¼ ê²€ì¦
        assert os.path.exists(crawler.output_dir)
        # ì €ì¥ëœ íŒŒì¼ í™•ì¸ ë“±
```

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìƒˆ í¬ë¡¤ëŸ¬ ê°œë°œ ì™„ë£Œ ì „ í™•ì¸ì‚¬í•­:

- [ ] `BaseCrawler`ë¥¼ ìƒì†ë°›ê³  í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„
- [ ] ì„¤ì • í´ë˜ìŠ¤ êµ¬í˜„ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [ ] `pyproject.toml`ì— Entry Point ë“±ë¡
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹… êµ¬í˜„
- [ ] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (`_cleanup` ë©”ì„œë“œ)
- [ ] ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
- [ ] ì¦ë¶„ í¬ë¡¤ë§ ì§€ì› (í•„ìš”í•œ ê²½ìš°)
- [ ] ë¬¸ì„œí™” (ì£¼ì„ ë° docstring)

## ğŸš€ ë°°í¬ ë° ì‹¤í–‰

```bash
# ì˜ì¡´ì„± ì¬ì„¤ì¹˜
poetry install

# ìƒˆ í¬ë¡¤ëŸ¬ ì‹¤í–‰
poetry run start new_site

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
poetry run pytest tests/test_new_site.py -v
```

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼í•˜ë©´ DDOBAK RAG Source Collectorì˜ ì•„í‚¤í…ì²˜ì— ë§ëŠ” ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 